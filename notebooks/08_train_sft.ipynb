{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f3d70",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from models.dataset import EDCopilotDataset\n",
    "from models.ed_copilot_sft import EDCopilotSFT\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "# Configura√ß√µes (conforme Tabela 8 do paper)\n",
    "CONFIG = {\n",
    "    'model_name': 'microsoft/BioGPT',\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 1e-5,\n",
    "    'epochs': 15,\n",
    "    'warmup_percentage': 0.1,\n",
    "    'weight_decay': 0.01,\n",
    "    'class_weight': 10.0,\n",
    "    'max_length': 656,\n",
    "    'save_dir': 'models/checkpoints/sft'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c73ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        # Move to device\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                 for k, v in batch.items()}\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "        loss_dict = model.compute_loss(outputs, batch, CONFIG['class_weight'])\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict['loss'].backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Log\n",
    "        total_loss += loss_dict['loss'].item()\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss_dict['loss'].item(),\n",
    "            'loss_lab': loss_dict['loss_lab'],\n",
    "            'loss_outcome': loss_dict['loss_outcome']\n",
    "        })\n",
    "        \n",
    "        wandb.log({\n",
    "            'train/loss': loss_dict['loss'].item(),\n",
    "            'train/loss_lab': loss_dict['loss_lab'],\n",
    "            'train/loss_outcome': loss_dict['loss_outcome'],\n",
    "            'train/lr': scheduler.get_last_lr()[0]\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                     for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "            loss_dict = model.compute_loss(outputs, batch, CONFIG['class_weight'])\n",
    "            total_loss += loss_dict['loss'].item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a060b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"ed-copilot-tcc\", config=CONFIG)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# Datasets\n",
    "print(\"üìÇ Loading datasets...\")\n",
    "train_dataset = EDCopilotDataset(\n",
    "    'data/processed/linearized/train.parquet',\n",
    "    CONFIG['model_name'],\n",
    "    CONFIG['max_length']\n",
    ")\n",
    "val_dataset = EDCopilotDataset(\n",
    "    'data/processed/linearized/val.parquet',\n",
    "    CONFIG['model_name'],\n",
    "    CONFIG['max_length']\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab36e9de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "print(\"üèóÔ∏è Building model...\")\n",
    "model = EDCopilotSFT(CONFIG['model_name']).to(device)\n",
    "\n",
    "# Resize embeddings se adicionamos [EOS]\n",
    "model.backbone.resize_token_embeddings(len(train_dataset.tokenizer))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * CONFIG['epochs']\n",
    "warmup_steps = int(CONFIG['warmup_percentage'] * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50baf88",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"\\nüéØ Starting training...\")\n",
    "best_val_loss = float('inf')\n",
    "save_dir = Path(CONFIG['save_dir'])\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    print(f\"\\nüìÖ Epoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    val_loss = validate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train/epoch_loss': train_loss,\n",
    "        'val/epoch_loss': val_loss\n",
    "    })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': CONFIG\n",
    "        }, save_dir / 'best_model.pt')\n",
    "        print(f\"‚úÖ Saved best model (val_loss: {val_loss:.4f})\")\n",
    "\n",
    "print(\"\\nüéâ Training complete!\")\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
