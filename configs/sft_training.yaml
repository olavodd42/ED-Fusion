# Configuração para Supervised Fine-Tuning

training:
  epochs: 10
  batch_size: 16
  learning_rate: 5e-5
  warmup_steps: 500
  weight_decay: 0.01
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  
optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1e-8

scheduler:
  type: "linear"
  num_warmup_steps: 500

logging:
  log_interval: 100
  eval_interval: 500
  save_interval: 1000
  tensorboard: true